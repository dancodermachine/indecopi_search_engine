{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6914112",
   "metadata": {},
   "source": [
    "# Title: INDECOPI Search Engine Using NLP\n",
    "## Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9300a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Daniel Villanueva'\n",
    "__email__ = '2144810@brunel.ac.uk'\n",
    "__website__ = 'https://www.linkedin.com/in/danielvillanuevanunez/'\n",
    "__copyright__ = 'Copyright 2022, Daniel Villanueva'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b224161",
   "metadata": {},
   "source": [
    "## 2. Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fac4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import pickle\n",
    "import string\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "seed=101096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bb2a2",
   "metadata": {},
   "source": [
    "## 3. Cleaning & Building The TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52688e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting directory of the \"resoluciones\"\n",
    "resoluciones = [\"resoluciones/\" + filename for filename in os.listdir(\"resoluciones/\")]\n",
    "\n",
    "# List of stopwords in Spanish.\n",
    "stopword_es = nltk.corpus.stopwords.words('spanish')\n",
    "# Add the \"\\n\" in the list.\n",
    "stopword_es.append(\"\\n\")\n",
    "# List of punctuations such as \".\", \",\". \"\\\".\n",
    "punctuation = [punct for punct in string.punctuation]\n",
    "# Combine both lists into only one.\n",
    "undesirable_values = punctuation + stopword_es\n",
    "\n",
    "# Instantiating the Stemmer class to stem Spanish Words.\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "# Instantiating the TF-IDF vectorizer.\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd7e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(list_resoluciones):\n",
    "    \"\"\" This function converts the pdfs to text.  \n",
    "    Args:\n",
    "        * list_resoluciones (list): this list contains the relative location of the files. \n",
    "    Returns:\n",
    "        * corpus (list): this list contains the string version of the pdfs.\n",
    "    \"\"\"\n",
    "    # Empty list where documents will be stored.\n",
    "    corpus = []\n",
    "    # Iterating over files.\n",
    "    for file in list_resoluciones:\n",
    "        # Open the file.\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            doc = \"\"\n",
    "            # This loop merges the pages of 1 document into 1 string.\n",
    "            for page in pdf.pages:\n",
    "                doc += page.extract_text()\n",
    "            # Merged doc gets saved in the list.\n",
    "            corpus.append(doc)\n",
    "    return corpus\n",
    "\n",
    "def cleaning_corpus(corpus):\n",
    "    \"\"\" This function cleans the corpus by removing undesirable strings (e.g., \"/\", \".\", \"\\n\") and stems the words\n",
    "        inside.\n",
    "    Args:\n",
    "        * corpus(list): list containining the documents that needs to be cleaned.\n",
    "    Returns:\n",
    "        * corpus_clean(list): list containining the cleaned documents.\n",
    "    \"\"\"\n",
    "    # Empty list where cleaned documents will be stored. \n",
    "    corpus_clean = []\n",
    "    \n",
    "    # Iterating over the corpus\n",
    "    for doc in corpus:\n",
    "        # 1. Removing strings from the document that belong to the undersirable_values list.\n",
    "        # 2. Stemming each word.\n",
    "        words = [stemmer.stem(word) for word in word_tokenize(doc) if word not in undesirable_values]\n",
    "        # Joining the words into 1 document.\n",
    "        doc = \" \".join(words)\n",
    "        # Adding the document to the corpus.\n",
    "        corpus_clean.append(doc)\n",
    "        \n",
    "    return corpus_clean\n",
    "\n",
    "def vectorizing_tfidf(cleaned_corpus, name_resoluciones, tfidf_filename):\n",
    "    \"\"\" This function converts list of documents into an array holding the tf-idf values. Then this array is converted\n",
    "        into a dataframe. \n",
    "    Args:\n",
    "        * cleaned_corpus(list):\n",
    "        * name_resoluciones(list):\n",
    "    Returns:\n",
    "        * df_corpus(dataframe):d\n",
    "    \"\"\"\n",
    "    # Convert corpus list into tf-idf\n",
    "    X = vectorizer.fit_transform(cleaned_corpus)\n",
    "    # The names of the columns are the words. Each word is a feature.\n",
    "    column_names = vectorizer.get_feature_names_out()\n",
    "    # Create dataframe with the list containing the documents.\n",
    "    df_corpus = pd.DataFrame(X.toarray(), columns=column_names)\n",
    "    # Add the column \"label\" that contains the name of the files.\n",
    "    df_corpus[\"label\"] = name_resoluciones\n",
    "    # Save vectorizer model\n",
    "    pickle.dump(vectorizer, open(tfidf_filename, 'wb'))\n",
    "    \n",
    "    return df_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5d3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pdf_to_text(resoluciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a805f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean = cleaning_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ae62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_filename = \"model/indecopi_resoluciones_tfidf.sav\"\n",
    "df_corpus = vectorizing_tfidf(corpus_clean, resoluciones, tfidf_filename)\n",
    "df_corpus.to_csv(\"data/resoluciones_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58118b4",
   "metadata": {},
   "source": [
    "## 4. Ranking Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e561bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indesearch import INDESearch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaaf9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"no reembolso\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05636b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the tf-idf model.\n",
    "tfidf_filename = \"model/indecopi_resoluciones_tfidf.sav\"\n",
    "# Database\n",
    "df_corpus = pd.read_csv(\"data/resoluciones_tfidf.csv\")\n",
    "# Instantiating the class.\n",
    "query_class = INDESearch(query, top_values = 10, database = df_corpus)\n",
    "# Clean the query.\n",
    "clean_query = query_class.cleaning_query_tfidf(tfidf_filename)\n",
    "# Dataframe containing the results\n",
    "result = query_class.similarity(clean_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1bc58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resoluciones/doc_202010140846376795.pdf</td>\n",
       "      <td>0.020277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resoluciones/doc_202008260017144816.pdf</td>\n",
       "      <td>0.007481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resoluciones/doc_202008302319029384.pdf</td>\n",
       "      <td>0.004806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resoluciones/doc_202010212308215994.pdf</td>\n",
       "      <td>0.004543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resoluciones/doc_202011290056011368.pdf</td>\n",
       "      <td>0.003378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resoluciones/doc_202004281540131638.pdf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>resoluciones/doc_202008302340032467.pdf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resoluciones/doc_202012130051341411.pdf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>resoluciones/doc_202011081957579777.pdf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resoluciones/doc_202011081842348593.pdf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     label  similarity_score\n",
       "0  resoluciones/doc_202010140846376795.pdf          0.020277\n",
       "1  resoluciones/doc_202008260017144816.pdf          0.007481\n",
       "2  resoluciones/doc_202008302319029384.pdf          0.004806\n",
       "3  resoluciones/doc_202010212308215994.pdf          0.004543\n",
       "4  resoluciones/doc_202011290056011368.pdf          0.003378\n",
       "5  resoluciones/doc_202004281540131638.pdf          0.000000\n",
       "6  resoluciones/doc_202008302340032467.pdf          0.000000\n",
       "7  resoluciones/doc_202012130051341411.pdf          0.000000\n",
       "8  resoluciones/doc_202011081957579777.pdf          0.000000\n",
       "9  resoluciones/doc_202011081842348593.pdf          0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053723ba",
   "metadata": {},
   "source": [
    "## 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8f76c",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/4211209/remove-all-the-elements-that-occur-in-one-list-from-another\n",
    "* https://stackoverflow.com/questions/5618878/how-to-convert-list-to-string\n",
    "* https://discuss.streamlit.io/t/how-to-download-file-in-streamlit/1806/27"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
